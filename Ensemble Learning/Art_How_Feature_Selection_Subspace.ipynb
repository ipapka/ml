{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Subspace Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.794 (0.048)\n"
     ]
    }
   ],
   "source": [
    "# evaluate a decision tree on the classification dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# define the random subspace ensemble model\n",
    "model = DecisionTreeClassifier()\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Feature Selection Method Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA F-statistic Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.830 (0.044)\n"
     ]
    }
   ],
   "source": [
    "# example of an ensemble created from features selected with the anova f-statistic\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "\t# define the base models\n",
    "\tmodels = list()\n",
    "\t# enumerate the features in the training dataset\n",
    "\tfor i in range(1, n_features+1):\n",
    "\t\t# create the feature selection transform\n",
    "\t\tfs = SelectKBest(score_func=f_classif, k=i)\n",
    "\t\t# create the model\n",
    "\t\tmodel = DecisionTreeClassifier()\n",
    "\t\t# create the pipeline\n",
    "\t\tpipe = Pipeline([('fs',fs), ('m', model)])\n",
    "\t\t# add as a tuple to the list of models for voting\n",
    "\t\tmodels.append((str(i),pipe))\n",
    "\t# define the voting ensemble\n",
    "\tensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "\treturn ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(X.shape[1])\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.825 (0.045)\n"
     ]
    }
   ],
   "source": [
    "# example of an ensemble created from features selected with mutual information\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "\t# define the base models\n",
    "\tmodels = list()\n",
    "\t# enumerate the features in the training dataset\n",
    "\tfor i in range(1, n_features+1):\n",
    "\t\t# create the feature selection transform\n",
    "\t\tfs = SelectKBest(score_func=mutual_info_classif, k=i)\n",
    "\t\t# create the model\n",
    "\t\tmodel = DecisionTreeClassifier()\n",
    "\t\t# create the pipeline\n",
    "\t\tpipe = Pipeline([('fs',fs), ('m', model)])\n",
    "\t\t# add as a tuple to the list of models for voting\n",
    "\t\tmodels.append((str(i),pipe))\n",
    "\t# define the voting ensemble\n",
    "\tensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "\treturn ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(X.shape[1])\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.821 (0.042)\n"
     ]
    }
   ],
   "source": [
    "# example of an ensemble created from features selected with RFE\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "\t# define the base models\n",
    "\tmodels = list()\n",
    "\t# enumerate the features in the training dataset\n",
    "\tfor i in range(1, n_features+1):\n",
    "\t\t# create the feature selection transform\n",
    "\t\tfs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "\t\t# create the model\n",
    "\t\tmodel = DecisionTreeClassifier()\n",
    "\t\t# create the pipeline\n",
    "\t\tpipe = Pipeline([('fs',fs), ('m', model)])\n",
    "\t\t# add as a tuple to the list of models for voting\n",
    "\t\tmodels.append((str(i),pipe))\n",
    "\t# define the voting ensemble\n",
    "\tensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "\treturn ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(X.shape[1])\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Feature Selection Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble With Fixed Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.833 (0.038)\n"
     ]
    }
   ],
   "source": [
    "# ensemble of a fixed number features selected by different feature selection methods\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "\t# define the base models\n",
    "\tmodels = list()\n",
    "\t# anova\n",
    "\tfs = SelectKBest(score_func=f_classif, k=n_features)\n",
    "\tanova = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\tmodels.append(('anova', anova))\n",
    "\t# mutual information\n",
    "\tfs = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "\tmutinfo = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\tmodels.append(('mutinfo', mutinfo))\n",
    "\t# rfe\n",
    "\tfs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=n_features)\n",
    "\trfe = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\tmodels.append(('rfe', rfe))\n",
    "\t# define the voting ensemble\n",
    "\tensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "\treturn ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(15)\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">anova: 0.817 (0.039)\n",
      ">mutinfo: 0.802 (0.039)\n",
      ">rfe: 0.832 (0.037)\n",
      ">ensemble: 0.838 (0.042)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# comparison of ensemble of a fixed number features to single models fit on each set of features\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "\t# define the base models\n",
    "\tmodels, names = list(), list()\n",
    "\t# anova\n",
    "\tfs = SelectKBest(score_func=f_classif, k=n_features)\n",
    "\tanova = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\tmodels.append(('anova', anova))\n",
    "\tnames.append('anova')\n",
    "\t# mutual information\n",
    "\tfs = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "\tmutinfo = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\tmodels.append(('mutinfo', mutinfo))\n",
    "\tnames.append('mutinfo')\n",
    "\t# rfe\n",
    "\tfs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=n_features)\n",
    "\trfe = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\tmodels.append(('rfe', rfe))\n",
    "\tnames.append('rfe')\n",
    "\t# define the voting ensemble\n",
    "\tensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "\tnames.append('ensemble')\n",
    "\treturn names, [anova, mutinfo, rfe, ensemble]\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# get the ensemble model\n",
    "names, models = get_ensemble(15)\n",
    "# evaluate each model\n",
    "results = list()\n",
    "for model,name in zip(models,names):\n",
    "\t# define the evaluation method\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate the model on the dataset\n",
    "\tn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\t# report performance\n",
    "\tprint('>%s: %.3f (%.3f)' % (name, mean(n_scores), std(n_scores)))\n",
    "\tresults.append(n_scores)\n",
    "# plot the results for comparison\n",
    "pyplot.boxplot(results, labels=names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble With Contiguous Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.861 (0.038)\n"
     ]
    }
   ],
   "source": [
    "# ensemble of many subsets of features selected by multiple feature selection methods\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features_start, n_features_end):\n",
    "\t# define the base models\n",
    "\tmodels = list()\n",
    "\tfor i in range(n_features_start, n_features_end+1):\n",
    "\t\t# anova\n",
    "\t\tfs = SelectKBest(score_func=f_classif, k=i)\n",
    "\t\tanova = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\t\tmodels.append(('anova'+str(i), anova))\n",
    "\t\t# mutual information\n",
    "\t\tfs = SelectKBest(score_func=mutual_info_classif, k=i)\n",
    "\t\tmutinfo = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\t\tmodels.append(('mutinfo'+str(i), mutinfo))\n",
    "\t\t# rfe\n",
    "\t\tfs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "\t\trfe = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\t\tmodels.append(('rfe'+str(i), rfe))\n",
    "\t# define the voting ensemble\n",
    "\tensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "\treturn ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(1, 20)\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.846 (0.045)\n"
     ]
    }
   ],
   "source": [
    "# ensemble of many subsets of features selected by multiple feature selection methods\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features_start, n_features_end):\n",
    "\t# define the base models\n",
    "\tmodels = list()\n",
    "\tfor i in range(n_features_start, n_features_end+1):\n",
    "\t\t# anova\n",
    "\t\tfs = SelectKBest(score_func=f_classif, k=i)\n",
    "\t\tanova = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\t\tmodels.append(('anova'+str(i), anova))\n",
    "\t\t# mutual information\n",
    "\t\tfs = SelectKBest(score_func=mutual_info_classif, k=i)\n",
    "\t\tmutinfo = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\t\tmodels.append(('mutinfo'+str(i), mutinfo))\n",
    "\t\t# rfe\n",
    "\t\tfs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "\t\trfe = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "\t\tmodels.append(('rfe'+str(i), rfe))\n",
    "\t# define the voting ensemble\n",
    "\tensemble = StackingClassifier(estimators=models)\n",
    "\treturn ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(1, 20)\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: -2.520 (0.397)\n"
     ]
    }
   ],
   "source": [
    "# ensemble of many subsets of features selected by multiple feature selection methods\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RepeatedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif, f_regression, mutual_info_regression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import VotingRegressor, BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features_start, n_features_end):\n",
    "\t# define the base models\n",
    "\tmodels = list()\n",
    "\t# normalization\n",
    "\tnorm = Pipeline([('s', MinMaxScaler()), ('m', DecisionTreeRegressor())])\n",
    "\tmodels.append(('norm', norm))\n",
    "\tfor i in range(n_features_start, n_features_end+1):\n",
    "\t\t# anova\n",
    "\t\tfs = SelectKBest(score_func=f_regression, k=i)\n",
    "\t\tanova = Pipeline([('fs', fs), ('m', DecisionTreeRegressor())])\n",
    "\t\tmodels.append(('anova'+str(i), anova))\n",
    "\t\t# mutual information\n",
    "\t\tfs = SelectKBest(score_func=mutual_info_regression, k=i)\n",
    "\t\tmutinfo = Pipeline([('fs', fs), ('m', DecisionTreeRegressor())])\n",
    "\t\tmodels.append(('mutinfo'+str(i), mutinfo))\n",
    "\t\t# rfe\n",
    "\t\tfs = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=i)\n",
    "\t\trfe = Pipeline([('fs', fs), ('m', DecisionTreeRegressor())])\n",
    "\t\tmodels.append(('rfe'+str(i), rfe))\n",
    "\t# define the voting ensemble\n",
    "\tensemble = VotingRegressor(estimators=models)\n",
    "\treturn ensemble\n",
    "\n",
    "# import data\n",
    "url= 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "data=pd.read_csv(url, header = None)\n",
    "\n",
    "dataset=data.values\n",
    "# prepare data\n",
    "X = dataset[:,:13]#.reshape(-1,1)\n",
    "y = dataset[:,13]\n",
    "\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(1, 13)\n",
    "# define the evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: -2.368 (0.386)\n"
     ]
    }
   ],
   "source": [
    "# ensemble of many subsets of features selected by multiple feature selection methods\n",
    "from numpy import mean\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RepeatedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif, f_regression, mutual_info_regression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import VotingRegressor, BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "np.warnings.filterwarnings('ignore')\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features_start, n_features_end):\n",
    "\t# define the base models\n",
    "\tmodels = list()\n",
    "\t# normalization\n",
    "\tnorm = Pipeline([('s', MinMaxScaler()), ('m', XGBRegressor())])\n",
    "\tmodels.append(('norm', norm))\n",
    "\n",
    "\t# standardization\n",
    "\tstd = Pipeline([('s', StandardScaler()), ('m', XGBRegressor())])\n",
    "\tmodels.append(('std', std))\n",
    "\tpoly = Pipeline([('s', PolynomialFeatures(5)), ('m', XGBRegressor())])\n",
    "\tmodels.append(('poly', poly))\n",
    "\t# robust\n",
    "\trobust = Pipeline([('s', RobustScaler()), ('m', XGBRegressor())])\n",
    "\tmodels.append(('robust', robust))\n",
    "\t# power\n",
    "\tpower = Pipeline([('s', PowerTransformer()), ('m', XGBRegressor())])\n",
    "\tmodels.append(('power', power))\n",
    "\t# quantile\n",
    "\tquant = Pipeline([('s', QuantileTransformer(n_quantiles=100, output_distribution='normal')), ('m', XGBRegressor())])\n",
    "\tmodels.append(('quant', quant))\n",
    "\t# kbins\n",
    "\tkbins = Pipeline([('s', KBinsDiscretizer(n_bins=20, encode='ordinal')), ('m', XGBRegressor())])\n",
    "\tmodels.append(('kbins', kbins))\n",
    "\tfor i in range(n_features_start, n_features_end+1):\n",
    "\t\t# anova\n",
    "\t\tfs = SelectKBest(score_func=f_regression, k=i)\n",
    "\t\tanova = Pipeline([('fs', fs), ('m', DecisionTreeRegressor())])\n",
    "\t\tmodels.append(('anova'+str(i), anova))\n",
    "\t\t# mutual information\n",
    "\t\tfs = SelectKBest(score_func=mutual_info_regression, k=i)\n",
    "\t\tmutinfo = Pipeline([('fs', fs), ('m', DecisionTreeRegressor())])\n",
    "\t\tmodels.append(('mutinfo'+str(i), mutinfo))\n",
    "\t\t# rfe\n",
    "\t\tfs = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=i)\n",
    "\t\trfe = Pipeline([('fs', fs), ('m', DecisionTreeRegressor())])\n",
    "\t\tmodels.append(('rfe'+str(i), rfe))\n",
    "\t# define the voting ensemble\n",
    "\tensemble = VotingRegressor(estimators=models)\n",
    "\treturn ensemble\n",
    "\n",
    "# import data\n",
    "url= 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "data=pd.read_csv(url, header = None)\n",
    "\n",
    "dataset=data.values\n",
    "# prepare data\n",
    "X = dataset[:,:13]#.reshape(-1,1)\n",
    "y = dataset[:,13]\n",
    "\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(1, 13)\n",
    "# define the evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: -2.644 (0.202)\n"
     ]
    }
   ],
   "source": [
    "# ensemble of many subsets of features selected by multiple feature selection methods\n",
    "from numpy import mean\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RepeatedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif, f_regression, mutual_info_regression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import VotingRegressor, BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "np.warnings.filterwarnings('ignore')\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features_start, n_features_end):\n",
    "\t# define the base models\n",
    "\tmodels = list()\n",
    "\t# normalization\n",
    "\tnorm = Pipeline([('s', PCA()), ('m', DecisionTreeRegressor())])\n",
    "\tmodels.append(('norm', norm))\n",
    "\n",
    "\tfor i in range(n_features_start, n_features_end+1):\n",
    "\n",
    "\t\t# anova\n",
    "\t\tfs = SelectKBest(score_func=f_regression, k=i)\n",
    "\t\tanova = Pipeline([('fs', fs), ('m', DecisionTreeRegressor())])\n",
    "\t\tmodels.append(('anova'+str(i), anova))\n",
    "\t\t# mutual information\n",
    "\t\tfs = SelectKBest(score_func=mutual_info_regression, k=i)\n",
    "\t\tmutinfo = Pipeline([('fs', fs), ('m', DecisionTreeRegressor())])\n",
    "\t\tmodels.append(('mutinfo'+str(i), mutinfo))\n",
    "\t\t# rfe\n",
    "\t\tfs = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=i)\n",
    "\t\trfe = Pipeline([('fs', fs), ('m', DecisionTreeRegressor())])\n",
    "\t\tmodels.append(('rfe'+str(i), rfe))\n",
    "\t# define the voting ensemble\n",
    "\tensemble = VotingRegressor(estimators=models)\n",
    "\treturn ensemble\n",
    "\n",
    "# import data\n",
    "url= 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "data=pd.read_csv(url, header = None)\n",
    "\n",
    "dataset=data.values\n",
    "# prepare data\n",
    "X = dataset[:,:13]#.reshape(-1,1)\n",
    "y = dataset[:,13]\n",
    "\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(1, 13)\n",
    "# define the evaluation method\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
